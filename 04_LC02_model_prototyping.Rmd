---
title: "LC02 chl-a model prototyping"
author: "Sam Sillen"
date: "2023-03-17"
output: html_document
---

packages
```{r setup, include=FALSE}
library(tidyverse)
library(feather)
library(viridis)
library(sf)
library(rgdal)
library(maps)
library(magrittr)
library(purrr)
library(data.table)
library(tmap)
library(ggthemes)
library(dplyr)
library(ggplot2)
library(mapview)
library(fs)
library(httr)
library(leaflet)
library(lwgeom)
library(nhdplusTools)
library(USAboundaries)
library(foreign)
library(CAST)
library(caret)
library(sp)
library(xgboost)
library(Metrics)
library(parallelly)
library(doParallel)
library(httpgd)
library(ggpmisc)

knitr::opts_chunk$set(echo = TRUE)
```


```{r functions, echo=FALSE}
## Functions we will use later
# This function randomly samples match-ups across different locations, times, and concentrations for splitting training/validation data

holdout <- function(x) {

  x <- x %>%
  group_by(long_group, time_group) %>% #split up into spatial and temporal groups
  dplyr::mutate(mag = cut(value, quantile(
  x = value,
  c(0, 0.2, 0.4, 0.6, 0.8, 0.9, 1),
  include.lowest = T
  )),
  mag = factor(
  mag,
  labels = c( 0.2, 0.4, 0.6, 0.8, 0.9, 1)
  )) %>%
  ungroup()
  
  set.seed(22)
  
  train <- x %>%
  group_by(time_group, long_group, mag) %>%
  sample_frac(.9) %>% #90% of data will be used for training
  ungroup() %>%
  dplyr::mutate(.partitions = 1)
  
  validate <- x %>%
   anti_join(train) %>%
   dplyr::mutate(.partitions = 2)

  out <- train %>%
  bind_rows(validate) 
    
  return(out)
}
```

# Read in spatial data to filter matchup data to ROI
```{r chla}
# SF for HUC2 basins from USDA NHD WBD - includes Mid Atlantic (HUC2_02), Great Lakes (HUC2_04), Ohio (HUC2_05), Tennessee (HUC2_06) and Upper Mississippi (HUC2_07)
huc2_basins <- read_sf(dsn="C:/Users/samsi/OneDrive - University of Pittsburgh/HUC2_Merge", layer="HUC2_Merge") 
st_transform(huc2_basins, crs ="+proj=longlat +datum=WGS84 +ellps=WGS84 +towgs84=0,0,0")

huc2_basins <- huc2_basins %>% filter(name != 'Mid Atlantic Region')

# Load in LC02 matchup database
matchup <- read_feather("C:/Users/samsi/OneDrive - University of Pittsburgh/OhioRiver_SR/Data/LC02_matchup.feather")

matchup <- matchup %>% 
filter(harmonized_parameter == 'chl.a')  %>% 
mutate(uniqueID = row_number())

pnts <- matchup %>% # Create new df with matchup ID, lat and long columns
  select(uniqueID, long, lat) 

sp_pnts = st_as_sf(pnts, coords=c('long', 'lat')) 
st_crs(sp_pnts)=4326
sp_pnts = st_transform(sp_pnts, crs=st_crs(huc2_basins)) # Make points spatial and set coordinate system to same as the HUC polygon

#Perform intersection between HUC2 shapefile and matchup points
Int=st_intersects(sp_pnts, huc2_basins)
#Create a new column in the sp_pnts data frame and write intersection results to it
sp_pnts$Intersect=lengths(Int)>0 #Finds only countries where intersection is TRUE
#Combine intersection results with original matchup dataset
matchup <- cbind(matchup, sp_pnts$Intersect) 
#Rename newly added column 

matchup <- matchup %>% rename(Intersect = "sp_pnts$Intersect")
```

# Applying filters to improve model performance 
```{r}
# Finish cleaning the data by filtering out matchups that are outside of HUC boundaries and other filters
matchup_filter <- matchup %>%
rename(value = "raw_value") %>%
  filter(dswe == 1) %>% 
  filter(value < 200) %>% 
  filter(hillShadow ==1 | is.na(hillShadow)) %>%
  filter(value > 0.1) %>% # Minimum accepted chlorophyll value
  filter(Intersect == TRUE) %>% 
  filter(season == 'Summer') %>% 
  filter(pCount_dswe1 > 30) %>% 
  filter(characteristicName != 'Chlorophyll a, corrected for pheophytin') %>%
  filter(Surface_temp_kelvin > 270) %>%
  select(date:gn.gn, lat, long, uniqueID, hue, dw, saturation)

matchup_filter$hue <- as.numeric(matchup_filter$hue)
matchup_filter$dw <- as.numeric(matchup_filter$dw)
matchup_filter$saturation <- as.numeric(matchup_filter$saturation)

```

# Setting up base model (no ffs, hypertuning)
```{r}
# make splits for training/validation

df <- matchup_filter %>%
  mutate(lat_group = cut_number(lat, 2, right= F),
         long_group = cut_number(long, 2, right=F),
         date = lubridate::ymd(date),
         julian = as.numeric(julian.Date(date)),
         space_group = paste0(lat_group,long_group),
         time_group = cut_number(julian, 3, right=F)) %>%
        # value = log(value)) %>%
         holdout() %>% 
         ungroup() %>%
         as.data.frame() %>%
  filter_all(all_vars(!is.infinite(.))) %>%
  filter_all(all_vars(!is.nan(.)))

outliers <- read_csv("C:/Users/samsi/OneDrive - University of Pittsburgh/OhioRiver_SR/Models/outlier_raw_90_noMidAtl_100rounds_percentile.csv") %>% #from outlier test
filter(count > 5)

df <- df %>% filter(!uniqueID %in% outliers$uniqueID)

# training data #80% of data to training 
train <- df %>%
  filter(.partitions ==1) %>% 
  ungroup() %>%
  as.data.frame() %>%
  filter_all(all_vars(!is.infinite(.))) %>%
  filter_all(all_vars(!is.nan(.))) 


# validation data 10% of data to validation
validate <- df %>%
  filter(.partitions ==2) %>%
  ungroup() %>%
  as.data.frame()%>%
  filter_all(all_vars(!is.infinite(.))) %>%
  filter_all(all_vars(!is.nan(.))) 

# make validation row index so you can rejoin later
val.cols <- df %>% 
  filter(.partitions ==2) %>%
  ungroup() %>%
  filter_all(all_vars(!is.infinite(.))) %>%
  filter_all(all_vars(!is.nan(.))) 

#Select some spectral indices/bands to use as predictors in model. Consider looking at lit and adding a bunch more that are good for chl-a specifically. 
features_1 <- df %>%
  select(NR:gn.gn, GCI, IRG, SABI, KIVU, GB, GNDVI, EVI, KAB, KRL, Surface_temp_kelvin, dw, hue, saturation) %>%
  names(.)

# create cross validation folds for spatial-temporal cross validation
folds <- CreateSpacetimeFolds(train, spacevar = "long_group", timevar = "time_group", k = 2)

# set training parameters
train_control_final <- caret::trainControl(
  method = "cv",
  savePredictions = T,
  returnResamp = 'final',
  index = folds$index,
  indexOut = folds$indexOut,
  verboseIter = T,
  allowParallel = TRUE,
  p = 0.9 #90% of the data is used to predict the other 20%
  )


# pick some defaut hyperparameters. No hyperparameter tuning yet here
  
grid_final <-expand.grid(
  nrounds = 50,
  alpha = 0,
  lambda =1,
  eta = 0.3
)
# make a model
model <- caret::train(
  x = train[,features_1],
  y = train$value,
  trControl = train_control_final,
  tuneLength = 1, 
 # tuneGrid = grid_final,
  method = "xgbLinear",
   # preProcess = c('center', 'scale'),
   importance = T,
  verbose = TRUE
)

# use model to make predictions over validation data
pred<- predict(model, validate[,features_1])
actual <- (validate$value)
uniqueID <- val.cols$uniqueID

output <- tibble(Predicted = pred, Actual = actual, uniqueID = uniqueID) %>%
  mutate(Actual = (Actual), Predicted =  (Predicted)) %>%
  left_join(df, by="uniqueID") %>%
  mutate(residual = Actual - Predicted,
         year = year(date),
         month = month(date),
         obs = ifelse(abs(residual) > 15, "bad", "good"))


# calcualate error metrics 
evals <- output %>%
  mutate(Actual = (Actual), 
         Predicted = (Predicted)) %>%
  summarise(rmse = rmse(Actual, Predicted),
            mae = mae(Actual, Predicted),
            mape = mape(Actual, Predicted),
            bias = bias(Actual, Predicted),
            p.bias = percent_bias(Actual, Predicted),
            smape = smape(Actual, Predicted)) %>% view()

ggplot(output , aes(x = (Actual), y = (Predicted))) + 
  geom_point() +
  xlim(0, 200) +
  ylim(0, 200) +
  geom_abline(slope=1, intercept = 0, color = 'black')+
  xlab("Measured  (ug/L)") +
  ylab("Predicted (ug/L)")+
  labs(subtitle = "No AOI filter, no outlier detection, (< 200 ug / L )",title = 'Base Model')+
  stat_poly_eq() +
  theme_few() 
```

# FFS for selecting best combination of predictor variables 
```{r}
set.seed(10)

folds <- CreateSpacetimeFolds(train,
  spacevar = "long_group",
  timevar = "time_group" )
  
control <- trainControl(
  method = "cv",
  savePredictions = 'none',
  returnResamp = 'final',
  index = folds$index,
  indexOut = folds$indexOut,
  p = 0.8)
  
  ## Do initial feature selection with conservative hyperparameters
tuneGrid1 <- expand.grid(
  nrounds = 300,
  eta = .1,
  lambda = 0,
  alpha = 0)

# Set it up to run in parallel. This can take 1-2 days.
cl <- makePSOCKcluster(availableCores() - 4)
registerDoParallel(cl)

ffs <- ffs(df[,features_1], df$value, method = 'xgbLinear', metric = 'RMSE', tuneGrid = tuneGrid1, Control = control, verbose = T)

on.exit(stopCluster(cl))

ffsResults <- ffs$perf_all

# Save the results
write_feather(ffsResults, "C:/Users/samsi/OneDrive - University of Pittsburgh/OhioRiver_SR/Models/ffs_results__AOI_filter_outlier_raw_90_percentile.feather")
#
ffsResults %>%
  group_by(nvar) %>%
  summarise(RMSE = median(RMSE),
            SE = median(SE)) %>%
  ggplot(.) + geom_line(aes(x = nvar, y = RMSE)) +
  geom_errorbar(aes(x = nvar, ymin = RMSE - SE, ymax = RMSE + SE), color = 'red')

#ggsave(paste0('figs/rfeRMSE_', iter, '.png'), device = 'png', width = 6, height = 4, units = 'in')
```

# Hyperparamter tuning 
```{r}
#Sometimes I have to run this unregister doparallel function, because I get an error about an invalid connection due to a lingering parallel operation

#unregister_dopar <- function() {
#  env <- foreach:::.foreachGlobals
#  rm(list=ls(name=env), pos=env)
#}

#unregister_dopar()

ffsResults <- read_feather("C:/Users/samsi/OneDrive - University of Pittsburgh/OhioRiver_SR/Models/ffs_results__AOI_filter_outlier_raw_90_percentile.feather")

ffs_features <- ffsResults[ffsResults$RMSE == min(ffsResults$RMSE),] %>%
  dplyr::select(-c(nvar, RMSE, SE)) %>%
  slice_head(n = 1) %>%
  select(-var11) %>% 
  paste(.)

grid_base <- expand.grid(
  nrounds = seq(100,500,100),
  alpha = c(0.01, 0.1, 0.5, 1),
  lambda = c(0.01, 0.1, 0.5, 1),
  eta = c(0.05, 0.1, 0.3)
)

set.seed(10)

folds <- CreateSpacetimeFolds(train, spacevar = "long_group", timevar = "time_group" , k=5)

train_control <- caret::trainControl(
  method = "cv",
  savePredictions = T,
  returnResamp = 'final',
  index = folds$index,
  indexOut = folds$indexOut,
  allowParallel = TRUE,
  p = 0.9,
  )
  
base_model <- caret::train(
  x = train[,ffs_features],
  y = train$value,
  trControl = train_control,
  tuneGrid = grid_base,
  method = "xgbLinear",
  verbose = TRUE,
 # preProcess = c('center', 'scale'),
  importance = F
)

base_model$bestTune
 
train_control_final <- caret::trainControl(
  method = "cv",
  savePredictions = T,
  returnResamp = 'final',
  index = folds$index,
  indexOut = folds$indexOut,
  allowParallel = TRUE,
  p = 0.9,
  )
  
grid_final <- expand.grid(
  nrounds = base_model$bestTune$nrounds,
  alpha = base_model$bestTune$alpha,
  lambda = base_model$bestTune$lambda,
  eta = base_model$bestTune$eta
)

model <- caret::train(
  x = train[,ffs_features],
  y = train$value,
  trControl = train_control_final,
  tuneGrid = grid_final,
  method = "xgbLinear",
 # preProcess = c('center', 'scale'),
  importance = F
)
```
# Evaluate final model
```{r eval}
# evaulate the model
pred<- predict(model, validate[,ffs_features])
actual <- (validate$value)
uniqueID <- val.cols$uniqueID

output <- tibble(Predicted = pred, Actual = actual,uniqueID = uniqueID) %>%
  mutate(Actual = exp(Actual), Predicted = exp(Predicted)) %>%
  left_join(df, by="uniqueID") %>%
  mutate(residual = Actual - Predicted,
         year = year(date),
         month = month(date),
         obs = ifelse(abs(residual) > quantile(abs(residual), .7, na.rm=T), "bad", "good"))

# calculate error metrics 
evals <- output %>%
  mutate(Actual = (Actual), 
         Predicted = (Predicted)) %>%
  summarise(rmse = rmse(Actual, Predicted),
            mae = mae(Actual, Predicted),
            mape = mape(Actual, Predicted),
            bias = bias(Actual, Predicted),
            p.bias = percent_bias(Actual, Predicted),
            smape = smape(Actual, Predicted)) %>% view()

ggplot(output , aes(x = (Actual), y = (Predicted))) + 
  geom_point() +
  xlim(0, 200) +
  ylim(0, 200) +
  geom_abline(slope=1, intercept = 0, color = 'black')+
  xlab("Measured  (ug/L)") +
  ylab("Predicted (ug/L)")+
  labs(subtitle = "AOI filter, outlier detection, (< 200 ug/L)",title = 'FFS, hypertuned model')+
  stat_poly_eq() +
  theme_few() 

save(model, file = "C:/Users/samsi/OneDrive - University of Pittsburgh/OhioRiver_SR/Models/final_model_AOI_filter_90th_percentile.rds")
```

# Read in prediction data, calculate AOA (prototyping at the moment), then predict 
```{r}
load( "C:/Users/samsi/OneDrive - University of Pittsburgh/OhioRiver_SR/Models/final_model_AOI_filter_90th_percentile.rds")

# Our model can can only be applied to new data if these are similar to the training data.
# Because we have implemented this outlier code, I feel as if our prediction data will 
# not be the same as our training data, and our model evaluation results will be over - 
# optimistic in regards to it's ability to predict on new data. 

# My goal, is to figure out a way to make the prediction data similar in 'data space' to
# the data that has been filtered using the outlier detection method. 

# Here, I try the area of applicability method (https://besjournals.onlinelibrary.wiley.com/doi/full/10.1111/2041-210X.13650)

# However, it looks as if it isn't necessarily working as it should ; not a lot of points 
# are outside the aoa , and thus the evaluation of this new data still looks far worse when
# compared to the model evaluation results 

trainDI <- trainDI(model, df)

aoa <- aoa(new_data, model, trainDI = trainDI, variables = 'all', method = 'MD')
  
# the aoa is in an 'aoa' object, so you have to turn it into a matrix then a df 

aoa.df <-as.data.frame(as.matrix(aoa$DI)) %>% 
mutate(rowID = row_number())

new_data_aoa <- new_data %>% mutate(rowID = row_number()) %>% 
mutate(pred = predict(model, new_data[,ffs_features])) %>% 
left_join(aoa.df) %>% 
filter(V1 < 1) 

evals <- new_data_aoa %>%
  mutate(Actual = (value), 
         Predicted = (pred)) %>%
  summarise(rmse = rmse(Actual, Predicted),
            mae = mae(Actual, Predicted),
            mape = mape(Actual, Predicted),
            bias = bias(Actual, Predicted),
            p.bias = percent_bias(Actual, Predicted),
            smape = smape(Actual, Predicted)) %>% view()


ffs_evals <- ggplot(output , aes(x = (Actual), y = (Predicted))) + 
  geom_point() +
  xlim(0, 200) +
  ylim(0, 200) +
  geom_abline(slope=1, intercept = 0, color = 'black')+
  xlab("Measured  (ug/L)") +
  ylab("Predicted (ug/L)")+
  labs(subtitle = "No Mid - Atlantic Observations, Hypertuned model and FFS applied",title = 'FInal Model Evaluation')+
  stat_poly_eq() +
  theme_few() 

new_data_aoa2  %>% distinct(date, SiteID, value, .keep_all = TRUE)

aoa_evals <- ggplot(new_data_aoa , aes(x = (value), y = (pred))) + 
  geom_point() +
  xlim(0, 200) +
  ylim(0, 200) +
  geom_abline(slope=1, intercept = 0, color = 'black')+
  xlab("Measured  (ug/L)") +
  ylab("Predicted (ug/L)")+
  labs(subtitle = "Ohio Data (new data)",title = 'AOA Model Evaluation')+
  stat_poly_eq() +
  theme_few() 

ggarrange(ffs_evals, aoa_evals)
```

# Testing other methods to create 'area of applicability'
```{r}

# Basically what I need to do is make the model results look similar to model results with a new 
# data set. 

# The AOA didn't seem to work. I wonder if there is a way to apply the filters from the outlier 
# detection method to the new data. 

# For example. if the training feature is similar in data space to the data that was REMOVED with 
# the outlier removal method, then throw that data out from the new data (prediction data )

#########

# load in outliers 

outliers <- read_csv("C:/Users/samsi/OneDrive - University of Pittsburgh/OhioRiver_SR/Models/outlier_raw_90_noMidAtl_100rounds_percentile.csv") %>% 
filter(count > 5)

# proprotional similarity index method 

training <- df[,ffs_features]

new_data_ffs <- new_data[ffs_features]

ggplot(outlier_data, aes(x=R.BS))+
  geom_histogram(color="darkblue", fill="lightblue")


ggplot(df, aes(x=R.BS))+
  geom_histogram(color="darkblue", fill="lightblue")

# Just spit balling here, but I'm thinking, can you do something where it's like ok , the distribution
# of a training feature in the new data has to be distributed the same way as the feature in the 
# training data. Or something like, ok here is the mean, std deviation of a feature in the training
# data. The feature in the new data has to be within 2 standard deviations of the mean of the training
# data. 

training <- training %>% 
pivot_longer(cols = c("R.BS", "B.GN", "GR2", "SN", "G.BS", "Surface_temp_kelvin", 
"N.RG", "IRG", "N_R", "B.GS"), values_to = 'value', names_to = 'Predictor') %>% 
group_by(Predictor) %>% 
summarise(mean = mean(value), stdev = sd(value)) %>% 
mutate(range_min = (mean) - ((stdev)*2), range_max = (mean) + ((stdev)*2))

new_data <- new_data %>% 
filter(R.BS >= 0.2668 & R.BS <=1.0092) %>% 
filter(B.GN >= 0.0936 & B.GN <= 0.7759) %>% 
filter(GR2 >= -0.0034 & GR2 <= 0.0576) %>% 
filter(SN >= -25.0073 & SN <= 25.9097) %>% 
filter(G.BS >= 0.3982 & G.BS <= 1.3877) %>% 
filter(Surface_temp_kelvin >= 289.6866 & Surface_temp_kelvin <= 303.1723) %>% 
filter(N.RG >= 0.1979 & N.RG <= 1.0546) %>% 
filter(IRG >=-0.0229 & IRG <= 0.001) %>% 
filter(N_R >= -0.0113 & N_R <= 0.0318) %>% 
filter(B.GS >= 0.2668 & B.GS <= 1.0092)

new_data$pred <- predict(model, new_data[,ffs_features])

aoa_evals <- ggplot(new_data , aes(x = (value), y = (pred))) + 
  geom_point() +
  xlim(0, 200) +
  ylim(0, 200) +
  geom_abline(slope=1, intercept = 0, color = 'black')+
  xlab("Measured  (ug/L)") +
  ylab("Predicted (ug/L)")+
  labs(subtitle = "Ohio Data (new data)",title = 'AOA Model Evaluation')+
  stat_poly_eq() +
  theme_few() 

```

